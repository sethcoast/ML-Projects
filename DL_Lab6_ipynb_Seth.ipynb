{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab6.ipynb Seth ",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "## Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "## There are two parts of this lab:\n",
        "###  1.   Wiring up a basic sequence-to-sequence computation graph\n",
        "###  2.   Implementing your own GRU cell.\n",
        "\n",
        "\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7bdZWxvJrsx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "3947fc2a-485e-4b32-c8b8-504cbc890567"
      },
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "# file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file = unidecode.unidecode(open('./klyrics.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "print(n_characters)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-19 18:55:22--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 52.2.48.133, 3.214.17.10, 34.205.95.128, ...\n",
            "Connecting to piazza.com (piazza.com)|52.2.48.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2019-10-19 18:55:23--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 99.84.170.141, 99.84.170.226, 99.84.170.199, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|99.84.170.141|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "\r./text_files.tar.gz   0%[                    ]       0  --.-KB/s               \r./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-10-19 18:55:23 (25.4 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
            "file_len = 1765634\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TxBeKeNjJ0NQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "83a50754-b7fd-4746-8714-749cffe41dae"
      },
      "source": [
        "chunk_len = 100\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "na salute me or seduce me\n",
            "Indubitably, I'm too street--indubitably, I'ma do me\n",
            "Better than your bitch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "On0_WitWJ99e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d1ce9844-1258-4e89-f2c9-c59686cfa9a3"
      },
      "source": [
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return Variable(tensor)\n",
        "\n",
        "print(char_tensor('abcDEF'))\n",
        "print(all_characters[91])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n",
            "|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavAv50ZKQ-F",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(GRU, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.ones = torch.tensor(np.zeros(shape=(1,1,hidden_size)), dtype=torch.float32)\n",
        "        self.ones = self.ones.cuda(async=True)\n",
        "\n",
        "        # Define weight layers\n",
        "        self.W_xz = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.W_hz = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.W_xr = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.W_hr = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        # Initialize reset gate's (forget gate's) biases to 1, per the recommendation from lecture\n",
        "        nn.init.constant_(self.W_hr.bias, 1)\n",
        "        nn.init.constant_(self.W_xr.bias, 1)\n",
        "        self.W_xn = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.W_hn = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    # inputs will be an embedding of size [1,hidden_size]\n",
        "    def forward(self, inputs, hidden):\n",
        "        # Each layer does the following:\n",
        "        # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "        # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "        # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "        # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "        # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "        # 1. Update gate \n",
        "        z_t = torch.sigmoid(self.W_xz(inputs) + self.W_hz(hidden))\n",
        "        # 2. Reset Gate\n",
        "        r_t = torch.sigmoid(self.W_xr(inputs) + self.W_hr(hidden))\n",
        "        # 3. Current memory\n",
        "        n_t = torch.tanh(self.W_xn(inputs) + r_t*self.W_hn(hidden))\n",
        "        # 4. Final memory\n",
        "        hidden = (self.ones-z_t)*n_t + z_t*hidden\n",
        "\n",
        "        return hidden, hidden\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6tNdEnzWj5F",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        # more stuff here...\n",
        "        # The GRU's \"input_size\" is our hidden_size, because we've initialized the embedder to be of size \"hidden_size\"\n",
        "        self.GRU = GRU(self.hidden_size,self.hidden_size,num_layers=self.n_layers)\n",
        "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
        "        self.relu = nn.ReLU() # why use this?\n",
        "        # This squishes the output from hidden_size to output_size giving us a vector representation of our prediction\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size) \n",
        "\n",
        "\n",
        "    def forward(self, input_char, hidden):\n",
        "        # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "        # of the GRU\n",
        "        # Get char embedding\n",
        "        input_embedding = self.embedding(input_char).view(1, 1, -1)\n",
        "        # Pass embedding through RelU for some reason (up to you whether you see better results or not)\n",
        "        input_embedding = self.relu(input_embedding)\n",
        "        # Compute output and new hidden (h_n)\n",
        "        output, h_n = self.GRU(input_embedding, hidden)\n",
        "        # Currently output is a vector of size hidden_size. We need to squish it to be the vocabulary (output) size\n",
        "        output = self.out(output)\n",
        "        return output, h_n\n",
        "\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrhXghEPKD-5",
        "colab": {}
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ALC3Pf8Kbsi",
        "colab": {}
      },
      "source": [
        "# This architecture is set up to train on random chunks of text of length chunk_len (defined above)\n",
        "# Thus, you will write your training loop to iterate over each character in the chunks\n",
        "def train(inp, target):\n",
        "    ## initialize hidden layers, set up gradient and loss \n",
        "    # your code here\n",
        "    ## /\n",
        "    decoder_optimizer.zero_grad()\n",
        "    hidden = decoder.init_hidden()\n",
        "    loss = 0\n",
        "\n",
        "    for char_x, char_y_truth in zip(inp, target):\n",
        "        # 1. Get Prediction\n",
        "        # char_y_hat is going to be a vector of length vocab, which should be a probability dist over the values\n",
        "        char_x = char_x.cuda(async=True)\n",
        "        hidden = hidden.cuda(async=True)\n",
        "        char_y_hat, hidden = decoder(char_x, hidden)\n",
        "\n",
        "        # 2. Compute Loss\n",
        "        char_y_truth = char_y_truth.cuda(async=True)\n",
        "        char_y_truth = char_y_truth.unsqueeze(0)\n",
        "        loss += criterion(char_y_hat.squeeze(0), char_y_truth)\n",
        "    \n",
        "    # 3. Compute Gradient (loss.backward)\n",
        "    loss.backward()\n",
        "    # 4. Update weights (step)\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder` then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "\n",
        "**DONE:**\n",
        "* Fill out the evaluate function to generate text from a primed string\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-bp-OZ1KjNh",
        "colab": {}
      },
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    ## initialize hidden variable, initialize other useful variables \n",
        "        # your code here\n",
        "    ## /\n",
        "    with torch.no_grad():\n",
        "        predict_str = prime_str\n",
        "        # convert prime_str into machine readable list of integer\n",
        "        prime_str = char_tensor(prime_str)\n",
        "        # Initialize hidden\n",
        "        hidden = decoder.init_hidden()\n",
        "\n",
        "        # loop over prime_str, throwing away the prediction and feeding the hidden state \n",
        "        # through the model through over course of iteration through prime_str\n",
        "        for char_x in prime_str:\n",
        "            # Throw away prediction and feed hidden through the GRU\n",
        "            char_x = char_x.cuda(async=True)\n",
        "            hidden = hidden.cuda(async=True)\n",
        "            _, hidden = decoder(char_x, hidden)\n",
        "\n",
        "        # finish generating output\n",
        "        # char_x should still be the last char of prime_str at the beginning of the loop\n",
        "        for i in range(len(prime_str), predict_len):\n",
        "            # 1. Get prediction for the next char\n",
        "            char_x = char_x.cuda(async=True)\n",
        "            hidden = hidden.cuda(async=True)\n",
        "            pred, hidden = decoder(char_x, hidden)\n",
        "            # 2. Convert pred to probability distribution\n",
        "            # temperature represents how much to divide the logits by before computing the softmax.\n",
        "            pred = pred.squeeze(0)\n",
        "            pred = F.softmax(pred/temperature, dim=1)\n",
        "            # 3. Sample\n",
        "            pred_char = torch.multinomial(pred, 1)\n",
        "            # 4. Convert int to char\n",
        "            char = all_characters[pred_char.item()]\n",
        "            # 5. Append char to predict_str\n",
        "            predict_str += char\n",
        "            # 6. Set the input of the next pass to be the output of this pass\n",
        "            char_x = pred_char\n",
        "        \n",
        "        return predict_str\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n",
        "\n",
        "**TODO:** \n",
        "\n",
        "\n",
        "**DONE:**\n",
        "* Create some cool output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nXFeCmdKodw",
        "colab": {}
      },
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 1\n",
        "lr = 0.001\n",
        " \n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder.cuda()\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xKfozqw-6eqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07b41644-453d-43fc-e08f-42e5b0c4c2bc"
      },
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss_ = train(*random_training_set())       \n",
        "    loss_avg += loss_\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "        # print(\".8\",evaluate('Th', 100), '\\n')\n",
        "        print(\".6\",evaluate('[Verse 1: Kendrick]', 1000,.7), '\\n')\n",
        "        # print(\".5\",evaluate('Th', 100,.5), '\\n')\n",
        "        # print(\".2\",evaluate('Th', 100,.2), '\\n')\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[57.5126678943634 (200 4%) 280.9722]\n",
            ".6 [Verse 1: Kendrick]\n",
            "ger\n",
            "SWopkee, it ha o.dt corres the thed a tie de se ma\n",
            "t aan forour beo unge calk lind ray nd out neds ind se alll iond ipg been di'me caped gay lore ,ou the alin'd mon arge le to macer bered neere ce, hol her, at tho it ou're afling youl on tou b'ig Hoh I wan bere ali,' she cwer tha ad wocl Aodt athe gr guse bet at it hee\n",
            "sAn aill blo, tareis thou tee ad tthe thee tig and haet yis mive thers ime tCe g Hallk asur hamrore s tereHs e the at l wou tin pds us farringe sour on odle dceas thte te isr beat m imar theas r onr opend, f\n",
            "hallisC you ma8 amed meg mer mel Ve at houte shoure ycl olr'l bee sor, tr\n",
            "[ousg wad od soun' merhe mere the iftle\n",
            "Be nuror bel rian the pountuuth had ne mere ant Iot gowe miuss ither to and thaar come rirnagare our er ho and dor yorinr eere, [it he at ou tacss aneas tind'its ato nurat it ho, cear at wpacke sreet anr amm the rout theent y' kante ot bheas n fomu rece bora warlol.e im ou ute wit anrht panogs tewade\n",
            "y ulare sager mous baricng the  \n",
            "\n",
            "[109.79278612136841 (400 8%) 213.8565]\n",
            ".6 [Verse 1: Kendrick]\n",
            "\n",
            "[Versise bos tingher\n",
            "Be wan the ronge he wand to wehinig mat the heat it fumy the me whe sin gor dlond lacker pande sirou hed pand\n",
            "Timen boke inggive hy uand they igg htet th als the se the a ien ang wat her pathe toht sou the mer she care\n",
            "be se mes thacs fome fo the the dom hen the me the catt she pares\n",
            "\n",
            "[Verest sop be sat cack ing\n",
            "I memar the hat the fronge pans milt fun do eting ing gor and\n",
            "The mamerAe ou de what bend hemy fom cok tack on may the the the peatl mand the som t thigh oun deack she pepkingigh the cake pan bend, thit fuscer\n",
            "The asn ke pith tht or gow ham ing the the me pare the de -ore and andrick, andr boulln fhe she the parous nge, I you the cik phers okt fom meen me men comt whe he che winon' me thip rat tee Am p.ut thapr fack hen the and mit herith seimk mengis mon't sup, cam sot, hing be apin\n",
            "Bolee she the nto igh the lim wand\n",
            "The met tha sthas\n",
            "\n",
            "We the thige thasl the and she packe af lo we what he all oringhtes cad sit mar, Ganr I wingh the rot \n",
            "\n",
            "[162.18498134613037 (600 12%) 221.5641]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Lama care ding the go watin I geon te Ior docesin't for the cuse gith in of us noyre gatlicut\n",
            "I I' for ne Ree got, you ass\n",
            "I masin't you wand that the She bat ins\n",
            "Pus folik gan the pourdes seet is ou men thint ad in all mooy Rerl\n",
            "The ine maring an 'an mibe, yow hat the ast vele ganerahy I'm ad bour sake\n",
            "an forcin' up art fit peas\n",
            "I and the batls, the ceall ly sat on the she reat that ros gally has the you hat thathe sit haring is bode you mare than an git a shate the on wart maltes\n",
            "Beret in a mort watly, (illacad, you got sot pood thor lel at if freats the erin at the Sor ife thing I ald cacke dor acanod shathal ille yut wack the I frack lonlin' tust that be that and was wat in' dod watin's srey fic, Lalkn in a dyom wuidd or catrigt\n",
            "Thame up lak like nigga a in\n",
            "'th mar may mity it hatt on't hit shrip\n",
            "ad allky sowa\n",
            "Pin an the drit the wand geat I my dop\n",
            "Taml bam, and ice the pelh jatg my his I't plate fon at o lik gomt\n",
            "Is all weve teat pear tihe roy gupts pon thage s \n",
            "\n",
            "[214.56625247001648 (800 16%) 203.3425]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Yend the bon the lithe in you I the sour you ricke a dige the dase ceas the call that hat hor bal star hurd ot a for the sreen on it a sor your don't lime the ou te that ard gut you ring of gon to the son dap ren it amassentin' bou hem that like noo man ethe for se bale the you Gou Dor mon shor wo clace it capper yor pretany werete an my the aning it hor wit hon tina dor tico Cive cerel andr Lamp, of oustion, rigga, swor foucder yo us thit a faz cor And I shit thy gahe if my bic hee thoor the Gore the show syur stor deen your sor on to! right ron thon I frack\n",
            "And thin, Mon bow the wee on in the you babutre Thot you che mady 'ame sof for chon'm be I me cour coon the life\n",
            "Rin, when I'm my ziges, I frill anda the rounte solll tho you ne a be shom of the and win it that of the p out\n",
            "I tighte the an may you a my thikon the the by nor reptin\n",
            "Cage luke and tect\n",
            "We mok bat aith so holk tat a for com\n",
            "Fe the he puck to in trem sowie lile and the thas hor fracut I me the the m \n",
            "\n",
            "[267.1951313018799 (1000 20%) 213.9370]\n",
            ".6 [Verse 1: Kendrick]\n",
            "I go that I'm see do thing it cors I ste mee lowe and Kennick Live sis shis it on the like you rigga, callige!\n",
            "I and yarina it ang the buck pacaling loive in cand me Singa on the willl this pontine\n",
            "I live of thing that ite sorice Ligin is the how, in your and to the shigh loref rolus hen me my on trigga gan\n",
            "Hot bmyin' momin\n",
            "thy mas me win got bate in now then sop hant ges\n",
            "The ceall maring liming the my on ontick kenict me I conicta you freachice an she men of a if a the is diggas, I a gigga ith\n",
            "I got the someturs the thes kniggas, a shit in my (Hon cored yut liste diget King the ins that mast and but contery\n",
            "Thon an thit in you nadeys\n",
            "This me nigga, tho she with\n",
            "Futt the puttin in ging in ald and that wile diggan in hat jor four me me wang me chise the my mot you nen cong the fuck ting liggs but care watod you neereren, jut llick in my I go the nit the iine\n",
            "Ofe to me or the you and uptare gom, what mishit me sot you ald nigging mon, hard ur to we in corsing the she  \n",
            "\n",
            "[319.72740054130554 (1200 24%) 231.9804]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Lamar]\n",
            "Ad I'm a reorn a plasothen when I the buckin' derendick Lamar]\n",
            "Wef to the gont you ris if 2 Jyoun, the an that bakin' malining is dotch feare the a cunt Cat the mathin a pith yout reas tald that bun mad bock that the dongh masin't pat sut that heat git\n",
            "But it you dame thit bat the couft that a preeng of telly when when redme\n",
            "But bod smic\n",
            "Patin'\n",
            "Sup sent with the spand like simes\n",
            "That'll that whe got't it the be\n",
            "I pome lout up arntin gon that bat bey he\n",
            "Do got kell I got a and cort\n",
            "Wealline yu'm E.\n",
            "Then the it a call a propping in the a condner get wont then I'm the me for now say\n",
            "But 1 feren the scame of for that got but wat it noloding and you eastiin' bick Lamy laiting\n",
            "Im and when when I Gick on atatond hont the shompor]\n",
            "And you and reat I for to the like Jastan wayn it rond I till a cans bey gon't kand when I that the pHon't my a prith, the bap in that put then a got u[\n",
            "I drod I'm somethor from in of I bat a bithiang male m, I and wan got tat ant thoud tri \n",
            "\n",
            "[372.231317281723 (1400 28%) 187.2411]\n",
            ".6 [Verse 1: Kendrick]\n",
            "\n",
            "[Hook]\n",
            "Ad ig a sreep\n",
            "Holk enget your fur sasid\n",
            "Emarse searsiste can like the the alley the peal the we pele the fand ind it warked the broy\n",
            "And I got arnd my you that on ging the feech and a nigga may of the track raluch the of like in the my that's Co le fee the thee there, my and on the stripe weac\n",
            "And mury baght oul\n",
            "Od that me the reac, I don the damn the all nigga miet, botst a rame, And is dight liking that in thing bittiet the men the with and banger, I'm furn the whene masing dake mectant have my whatty yalke when you a philk a like 'light sot, the you right\n",
            "Oun in you geet you and I wand frode pester the nigga\n",
            "(And whey ave the I be heing of up Riht my wen the ceefice\n",
            "You and that the srestere the walkin' the fuck in you the with be alkin' a you gat the a scitur the you ge was sin The fater we thee wean fight tim trasten you rant, gon of you femipes\n",
            "And I leed is the like ap campeter\n",
            "A it a and like the bay\n",
            "How you rand bligh that you streep me the som your \n",
            "\n",
            "[425.0196945667267 (1600 32%) 169.5152]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Lamare for a Knock\n",
            "Whe what kendreck a sadustere\n",
            "An the preal co?\n",
            "Ho wer aboure a parce jut she where and make here whe a never she russ word it Aboud a like\n",
            "\n",
            "[Bren Cpreese ever be-chesser, up her some look, uppore Ron a lever the need a back\n",
            "I see see pelle feen caste for the smeed his neodd, Gos that's the now and the me ploid or the moutic homere hower\n",
            "I about the strodi\n",
            "I coned me Do come teel a suter the some pelor coke tway as me this bre a conet in And be my coullad the be albout one the me the shome now now my leochore Rown't a vered s as of the she\n",
            "You this ab\n",
            "I backe\n",
            "The you beelt to of on the gort the costed ne cramie\n",
            "U om car it le balls, some there, I nest is a cound (Houllets the show the you the some a be can the preacs and gate\n",
            "Hold my for wore how Dame le as this me ornowerys a the sto mall it for mous the lide streal so dake it\n",
            "I'll alley\n",
            "Shit mane one the wate rap cana\n",
            "Alight the sruck, I bep the git, facke wea sem of me?\n",
            "\n",
            "[Verse 2]\n",
            "So I wan me go \n",
            "\n",
            "[477.63629508018494 (1800 36%) 191.7215]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Ippest and sheel and, deston my pall you\n",
            "'m all we gust the butt ap\n",
            "Kendrick And just was I the thood thak beats it to seak, thhe shit\n",
            "On my smay sould polly ast got ming your for my momives yourr pook, beramly 'mould I'm goo it, sell it's all why way you luk nigga,, you likin' houl all moy shit my got ave shout wit's we cop-ingonting atousang, and in you sey mall the stay\n",
            "Whis say stell your shorey I neever you scropl like I stall my or the my shat uppony lalk when buct can but bitchter the brack (wah-'R\n",
            "I how my word\n",
            "I hade hit for the me how I nothilving usting, you\n",
            "\n",
            "[Sith, bes blow, never what boy my ags this\n",
            "Qad these we hacis, pook, that my shome to to fell the buck with hay shated shot bitchas the blow the like sell\n",
            "I speel bitch, I hod een and my stack\n",
            "Coull a shit\n",
            "I got the pull a bem you nigga\n",
            "Nod I say real mouch dish stad for all the pored thes and all my sleest\n",
            "Ast it the wovel your show do all you\n",
            "\n",
            "[Verse 2]\n",
            "When I I'm sing fitty, hongs when all I'm co \n",
            "\n",
            "[529.6966233253479 (2000 40%) 185.9833]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Lant it, dank\n",
            "And I saul she pontch of what's aland eex me and pose\n",
            "The po litte of the wan flaw you lack in the some the nigga\n",
            "The your gon fares feel I danging and noight\n",
            "I ne aring and a cripler\n",
            "I fring then got in a blath this and that ya could I dind striped the sciin him want and the was fugin' somuring your mome sisetch the with it mind evic un flack bitch to somen coned fright\n",
            "Nou dock watting take sit on that bloys love wone sin it flace I now that my moll the lith and on the buling in it\n",
            "[Verse 1]\n",
            "I nigga, not dir with the stake in high funt of your alin' wan rap the file want I don't eary som the hus some the paye\n",
            "I to wen I dempen got my likin' in he reen see litch\n",
            "Bitch see five kin't neva in sum hot mank fall ya kirl and I then fas curst an shit the post traking wome I fast eever brould and don't a merong never got of your sin to me hoad of somptin\n",
            "Beatin' f-fees and I go the blan be get we in sy nigga, me sack bitch tone fist the gos in they shinin' n \n",
            "\n",
            "[582.1854836940765 (2200 44%) 217.9088]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Night what's Loart you'll a steet hopping of be off ric toing you'll preed\n",
            "They was a beeld on throught\n",
            "Whey you defeetin' play sow we bitch, the plothong topppend that tomesens\n",
            "You my do\n",
            "So my he she you got watpers\n",
            "Gotter reade\n",
            "You peate the shough\n",
            "What when you the is the and the polly my shit I wee deed the poded you woop on to conces\n",
            "You'll ken hew the sell when my compton, sirt\n",
            "When you sol I don't murd off the proy\n",
            "The a spiting Compot, constrod\n",
            "Couct my piret\n",
            "I look wany me ting fomp\n",
            "Moth off, my foth\n",
            "I't know\n",
            "I prow be loked to fool me poster, like you when me\n",
            "Jout better take you so bright\n",
            "And kistic's 200 you woon\n",
            "\n",
            "[Verse 3]\n",
            "You got con what you what to my well a bitch\n",
            "Veroding to gont, you notton home spet hith you pooth, nother\n",
            "I the was arous\n",
            "That's yeouh\n",
            "Tell mank tee troked than you got the of she street hand on fick you to weetther never to then hee, me to gon't domethring you popporly of you ponetty bet fourt\n",
            "I don't to they but sell and the stan y \n",
            "\n",
            "[634.4371268749237 (2400 48%) 182.8724]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Lact if she hoote, I don't deen a hoppel to be whated's\n",
            "Semprood: realin'\n",
            "\n",
            "[Verse 1]\n",
            "\n",
            "[Verse 3]\n",
            "I don't pare\n",
            "I shat the restit to nogga, for that a madgurst\n",
            "Pould he shooting me like it it\n",
            "Pound it the chorin' a is him ming, with of the hit and tho cook babout bood the wheruth\n",
            "A do (Drask a be like to the gon' cunntin' on all the fing you so prest a be caup out it wake reopl what I what you gront that dut it\n",
            "But mysen a in through shit it stame a rippers\n",
            "\n",
            "[Verse 1]\n",
            "But had\n",
            "That's no' like a brothtin' call to that to be that chaded a in the really\n",
            "But and I realler about the the starst hat the it\n",
            "The I'm beat the for the gromis\n",
            "Tho t'p that's couke treakin' in it be ot Pame\n",
            "Soothin'\n",
            "Everybend it it it, the nigga\n",
            "I to the outon for be to calle in about the sell the strant, I not the pop, I what't to straop, go and don't you reats real he barts\n",
            "\n",
            "[Verse 13:DAK0rick Lamar]\n",
            "And the call of on to this a gontrant on the mord on foe\n",
            "Sat best a be the got shoxsion\n",
            "But shought \n",
            "\n",
            "[686.8723442554474 (2600 52%) 162.9909]\n",
            ".6 [Verse 1: Kendrick]\n",
            "What's be a mond\n",
            "Neal that me something awout that esteren I'll manna like that in what bock that everyming boott the come that's sully to a wa dolling mer that I'm a pary\n",
            "Not that fige the shit\n",
            "In the be pillal\n",
            "Out and what I'm the propes\n",
            "\n",
            "[Outro: Kendrick Lamar]\n",
            "I'm a know this himacre the for, my back hover chirs\n",
            "I vain hattromed\n",
            "That I'm treake bear up backing beten wher that in shit\n",
            "That the and black I'm the 'lock wond\n",
            "The for up sometone\n",
            "This has wan't ruins that I'm a from this a bere this saik\n",
            "\n",
            "[Verse 1: Kendrick Lamar]\n",
            "This a minde but this the came that saighatter that\n",
            "I'm starking that than roon\n",
            "I dong to korther cone, that's all my your said, no to dusty chade pussing, that that don't like a lorind when I'll tle hocketing a ceut and with the her there when I what that she somethin' out, the wack, I'm like beting me thap that's the ripping hand, shot that in the some\n",
            "So my spare that a smothing like a killas, be the chare like a it's you lake and love my \n",
            "\n",
            "[739.5224440097809 (2800 56%) 194.3189]\n",
            ".6 [Verse 1: Kendrick]\n",
            "\n",
            "\n",
            "WC\n",
            "Wak, posted back in ther coos shat my nitt Eat (Dat spit notts ar got thed mat nit hop yout shit frot shitne cor ear ach\n",
            "Sot fal wot dor are a the thir littco for ont to gactch strol there the thin Mtak blac sheed or shat ainn I pat the street on ing asar a wit, bal or ther theed far the theese cres\n",
            "Beon een there en I aceth for that a ning bletch enet hat neve what youneng thout a copt of whet your fllaind at a shot to sterat onthe stras in' pracer\n",
            "I chat I woars at war then hort chat ever dorse tat youn at chot all feed heet\n",
            "It bost a wrak net youn whoret on a noft of blad that in I ther lot thate wen ther got shit\n",
            "\n",
            "Thome youn feck me storte\n",
            "But yor to Dal what paf yourt nere that nott geen hat Golling pol to far micten yor cres al that mame fact\n",
            "A. an cal a cak sugn thereed fat fin the got clearca dom yor stat gst thess ontet at's cohut Tal that there Got, jumt monetcer I liff ag Cral wakcen houtho hher chere bat on there got tat a beal blay that the chore s \n",
            "\n",
            "[792.0298035144806 (3000 60%) 200.8167]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Yough I fake you the rung you froming promdos the scark to lisses, I'm dround they ge to cont but hough get's is strime, of I sture with star, I'm to was haid got the shit dide you froke you chas gon dy got for was the sae day\n",
            "Thes my beat\n",
            "Hook ya wort sady they from hough you on to never stanges\n",
            "The for it when homies a frow in the pessy all Gight goad wam beayled that shot rething a shirt\n",
            "So she suge you case what the rether feed my son pust poy see got? in with and been out gotts on, strote\n",
            "The mayright everysay shood deass\n",
            "So gong you and at the cack a my in steee you not best the right, when homey, housh the sissing ard on, I get you hund wwas no my for hoe digisin't if you forn mike see three fromta never you right to done? what this I for feel my like you when I sway, I'm stind wheen this ferester like you when youtight shat you Ket dide you the prean' juss do was fuctidin' was Bade on to shordes steeing they the ge Potire is to sangless, at the gomn the and  \n",
            "\n",
            "[844.5308372974396 (3200 64%) 189.2530]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Put be well a black and allamit\n",
            "I slaid up alls my like partich and I didn't love wanna blad a love the crepling you get it your hot I come the scark\n",
            "That us really in the Acan't you all the scarder of the grave misial\n",
            "Every packers, in you here him shit frow the shing fiulle!\n",
            "(Befled uck starth in the get my biblors doin' the got in too shit where my allise you shead my ding\n",
            "In the brealte\n",
            "Scmhan I life wetry wans I blong I cnotiriin a Blow\n",
            "Sto load in she peccty what you back\n",
            "Fike you the meen\n",
            "I all\n",
            "And sinte\n",
            "And gat Bust\n",
            "In in the pefton your back you Veall it a puss Paroung you cones gott and some, the sitgy on the hought this whound in like youg it, ene, you what say\n",
            "kend you mean, bull the wall my all like my sing packesity nigga fick out then I got take the down dobliming shoullt, banto bitch an mistin' back all make it say cack in the toen\n",
            "I the wore\n",
            "Well mass will it some make but on man\n",
            "I all sitron of you to Ked, in the was all the would not to I with lev \n",
            "\n",
            "[897.4771847724915 (3400 68%) 213.7379]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Whe and a don't the carin gover then he live is all we mother\n",
            "But vies just the beach\n",
            "You ho gon' for the won't be till the with I when the peten a of mill, mansing all the resas eever my trink the stand for that me\n",
            "\n",
            "[Verse 1: Kendrick Lamar]\n",
            "The suck on the strets, shot, the conce we in the all you chuck mana like want the right, must and when you all knew fuck, I'm the sain this aid dreat on these hound fanters went so stand\n",
            "Or me in the hopp, the one out with a care\n",
            "God of the rout, all me five to I'm neat on see on a fell the as enter wat me with\n",
            "Nod like in the and be to enew\n",
            "So the are trance deale and that's bit for your hood\n",
            "\n",
            "[Broke like mek\n",
            "Swome of you\n",
            "And I me see tod thit\n",
            "\n",
            "[Verse 3: Kendrick Lamar]\n",
            "I'm Hist a love\n",
            "You we that she in't\n",
            "Kend yeard insix\n",
            "And what you got cacusicate, onside the fread I'll but it no reent\n",
            "DA niggas be flite\n",
            "I dervince when I dive of my frean me be tranta dind shit the be a was that whet did with my me gon' likin' manter the d \n",
            "\n",
            "[949.8866667747498 (3600 72%) 160.2232]\n",
            ".6 [Verse 1: Kendrick]\n",
            "So reless seen that love you gove that see with the gon' can that you flacking loil you wire a lover the histrever came hors there is was stake like my on the wat my sich the back the houng my syo this on the plocked to peoss a compion for duss of the musters pised to par the proming this sone of you's with is feel to calling like it eathe hom more front his we lome everything A Chirlens up on my that's shottile is gown mind of you know me is do you right the scaricmar eaztented can a some on the gas sifce ain't gunnines to tell me for was this storts\n",
            "(Hip, drank hat be there like you gon you right to lock it furn the dray a fuck to iss a pperses a riplest proing keay\n",
            "I niggas treap chin in the shootting at they you sip nigga\n",
            "I from the word scaugh your bout a the wat liking through a clomped me with the seen the Hitch, your fearters is take that it for a making you was wis mifter my back I down it\n",
            "Furn what a be.thing at that say some is sime?, us say with you get  \n",
            "\n",
            "[1002.2323648929596 (3800 76%) 138.2875]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Nixting amardor. I know a brean\n",
            "You wanks a lack I deots that's a nothed all dollect\n",
            "No veane takine\n",
            "I dead a litch like me cangaius and I'm a know\n",
            "I a sayinica\n",
            "Het the pick\n",
            "Then I last on the bout petben to they unt they for a poinion and the hone\n",
            "Every niggas to moter your feel see gotting fuck a bist to give get at can a go that's store be off the real\n",
            "That's you for me, you wastarits\n",
            "\n",
            "[Chorus: Kendricked her we got got the dirster and and Kendrick\n",
            "Lever up wone so some suck cass be in check\n",
            "Mated midobous, this dy got the be it, you making at the gotter a bill all me how and the sonations mars\n",
            "I need don't steen this a don't in I adure stare on that down I was yu'll to me ime\n",
            "I was the got be gon' beatien and I got the strick thone round the bottelter sall me sone he droppen my bally Compton and shouse to man ot mate compon me\n",
            "\n",
            "[Verse 2: Kendrick Lamar]\n",
            "Hat a of don't got I gott me with he be don't go cause on a in the botter comieg boring\n",
            "I like I day, \"You can \n",
            "\n",
            "[1055.083610534668 (4000 80%) 172.3855]\n",
            ".6 [Verse 1: Kendrick]\n",
            "Put man\n",
            "I got like ya choce tell all it the bitch that\n",
            "I now that styme it on amy good that wack what you never want the be sone ned that a real me is to her shity hank\n",
            "Pese the houre a poop lie, sweeps to Chers\n",
            "Beal compin' on up to haters\n",
            "I and I can the pone on the can a can, 'ma up\n",
            "Feals to and foor through sone that I go what you in my fack it, be, with the streets want to go wan that a rapperle the mance somines\n",
            "Louter dive and that breack it the seat\n",
            "The from the and I a sist to paring now can goon on my cure\n",
            "Ind a cread me and about nigga\n",
            "Pom the hand a all agnatam now, the can strindin' Laborther some nigga, the see pood somethin' X'm and my be reald, new on think it of eem what then chappin' wat and a retrother\n",
            "I we befer theg rade the go on the same he gon' can that be gon' flishing a recome I nened is is she hottricanded\n",
            "Tring that it, be ween be be coness\n",
            "Fece was al I good be to the resetens\n",
            "I said you and sone at to the conce street off the be in smol \n",
            "\n",
            "[1108.5670592784882 (4200 84%) 163.0838]\n",
            ".6 [Verse 1: Kendrick]\n",
            "\n",
            "[Herse 3: Kendrick Lamar for say got I don't be that matded all when the maring to line the hoin\n",
            "She'll gull if your not the litch these of love the sture\n",
            "We and nigga\n",
            "It's shit to everybody live and sywat, I'm that's a prome by can a blost in it pooning\n",
            "A shit my can it to have your street trings like lorf compon and I look so downt niggas\n",
            "I don't get make can at was niggas and I'm run congale\n",
            "And the my spit and kill in sypenter that I hould some to niggas when street to peter brying it for be can the blown\n",
            "Your fime Bimies\n",
            "May like heess shit down, you wasta doe, hardy back in all grastals sacire\n",
            "Wealin' like y'all, bitch, I get to shot streets\n",
            "\n",
            "[Intro: Kendrick Lamar pritrows bock can them on hew be shot is dishin'\n",
            "I reallowny say, I with her your hut, gom said one think these betting you real the reet, you and that get and how I got the scasin me wean mighes my nigga lookn\n",
            "I and a burt liek a streettal\n",
            "And I bay comppon them sit it heir of cintred my casas fuc \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ee0so6aKJ5L8",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 1000), '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpA6wqxWhcFx",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n",
        "I downloaded all of Kendrick Lamar's lyrics into a text file and ran my implementation of the GRU on the text. I primed the output on the string \"[Verse 1: Kendrick]\". Interestingly it learned that shorter lines tend to occur next to shorter lines, and longer lines tend to occur next to longer lines. The model's output isn't anything crazy impressive, with most lines containing mostly nonsense. But it's clear that the model learned to speak like Kendrick Lamar, using a fair number of curse words and properly capitalizing the word \"Compton.\" The most interesting thing to me is that it appears not only to have learned how to format song lyrics, but that there is some sequential rules in formatting song lyrics. [Verse 2] occurs after [Verse 1] and [Chorus] occurs between verses. (Note: periods between verses were added by me for the sake of formatting the output in this notebook. The actual model learned to output an additional newline character between verses and chorus)\n",
        "\n",
        "\n",
        "### Output 1000 Characters (After 18 minutes of training w/ Temperature .6)\n",
        "[Verse 1: Kendrick]\n",
        "\n",
        "Nixting amardor. I know a brean\n",
        "\n",
        "You wanks a lack I deots that's a nothed all dollect\n",
        "\n",
        "No veane takine\n",
        "\n",
        "I dead a litch like me cangaius and I'm a know\n",
        "\n",
        "I a sayinica\n",
        "\n",
        "Het the pick\n",
        "\n",
        "Then I last on the bout petben to they unt they for a poinion and the hone\n",
        "\n",
        "Every niggas to moter your feel see gotting fuck a bist to give get at can a go that's store be off the real\n",
        "\n",
        "That's you for me, you wastarits\n",
        "\n",
        ".\n",
        "\n",
        "[Chorus: Kendricked her we got got the dirster and and Kendrick\n",
        "\n",
        "Lever up wone so some suck cass be in check\n",
        "\n",
        "Mated midobous, this dy got the be it, you making at the gotter a bill all me how and the sonations mars\n",
        "\n",
        "I need don't steen this a don't in I adure stare on that down I was yu'll to me ime\n",
        "\n",
        "I was the got be gon' beatien and I got the strick thone round the bottelter sall me sone he droppen my bally Compton and shouse to man ot mate compon me\n",
        "\n",
        ".\n",
        "\n",
        "[Verse 2: Kendrick Lamar]\n",
        "\n",
        "Hat a of don't got I gott me with he be don't go cause on a in the botter comieg boring\n",
        "\n",
        "I like I day, \"You can"
      ]
    }
  ]
}